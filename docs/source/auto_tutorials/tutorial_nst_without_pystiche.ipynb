{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nNST via image-based optimisation without pystiche\n=================================================\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start of the tutorial by importing all necessary packages. Next to torch and\ntorchvision, we use PIL and matplotlib.pyplot to read, write, and display images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\nprint(\"I'm working with torch version \" + torch.__version__)\nfrom torch import nn\nimport torch.nn.functional as Fnn\nfrom torch import optim\n\nimport torchvision\n\nprint(\"I'm working with torchvision version \" + torchvision.__version__)\nfrom torchvision.models import vgg19\nfrom torchvision import transforms\nimport torchvision.transforms.functional as Fv\n\nfrom os import path\nfrom collections import OrderedDict\nfrom PIL import Image\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The NST algorithm involves a neural network as part of an optimisation problem. Thus\nit is really helpful to do all calculations on a GPU to speed up the process. This\ntutorial requires only about 2.5 GB of memory.\n\nFor this tutorial all operations are executed on a GPU if one is available. If that is\nnot the case it will still work correctly but significantly slower.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"I'm working on device: \" + str(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Sequential):\n    def __init__(self, *args):\n        super().__init__(*args)\n        self.eval()\n\n    def forward(self):\n        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we can finally dive into the actual NST, two more preliminary steps have to be\ntaken care of. The ``torchvision`` package already offers some transformations, but\nwe define some additonal ones.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The functionality of all transformations listed below could be achieved with the\n   ``transforms.Lambda()`` transformation. Unfortunately,\n   ``print(transforms.Lambda())`` would not display any information of what it is\n   doing. Since one of the goals of this tutorial is clarity, this is avoided here.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Transform(object):\n    def __repr__(self):\n        return \"{0}({1})\".format(self.__class__.__name__, self.extra_repr())\n\n    def extra_repr(self):\n        return \"\"\n\n\nclass ToCpu(Transform):\n    def __call__(self, tensor_image):\n        return tensor_image.cpu()\n\n\nclass EnforceFloatPixelValueRange(Transform):\n    def __call__(self, tensor_image):\n        return torch.clamp(tensor_image, 0.0, 1.0)\n\n\nclass AddFakeBatchDim(Transform):\n    def __call__(self, tensor_image):\n        return tensor_image.unsqueeze(0)\n\n\nclass RemoveFakeBatchDim(Transform):\n    def __call__(self, tensor_image):\n        return tensor_image.squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To apply all neceessary transformations conveniently, we bundle them together within\na ``transforms.Compose`` container. We define a ``preprocessor`` that performs the\nfollowing steps:\n\n1. Given an ``PIL`` image it is cast it into a ``torch.Tensor`` and the dimensions\n   are rearranged to ``CxHxW``.\n2. A fake batch dimensions is added to be able to pass the image into our encoder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preprocessor = transforms.Compose((transforms.ToTensor(), AddFakeBatchDim()))\nprint(\"I'm working with the following preprocessor:\")\nprint(preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``postprocessor`` is also defined as ``transforms.Compose`` and performs the\nsteps of the ``preprocessor`` in reverse as well as two additonal steps:\n\n1. The image is moved to the CPU before performing any other actions, since the\n   transformations are not defined to work on the GPU.\n2. Before converting the tensor back to a ``PIL`` image we enforce the float value\n   range for pixels. Since the optimization is unconstrained, it might have created\n   values outside the closed interval $\\left[ 0 ,\\, 1\\right]$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "postprocessor = transforms.Compose(\n    (\n        ToCpu(),\n        RemoveFakeBatchDim(),\n        EnforceFloatPixelValueRange(),\n        transforms.ToPILImage(),\n    )\n)\nprint(\"I'm working with the following postprocessor:\")\nprint(postprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a last preliminary step we define some image I/O functions that help us read,\nwrite, and show images. These helper functions incorporate the above defined\n``preprocessor`` and ``postprocessor`` so that we don't have to call them explicitly.\n\n:func:`read_image` resizes the input image so that the smallest side is\n``image_size`` pixels wide while keeping the aspect ratio constant. The default value\nis set to ``image_size=500`` since Gatys et. al. reported in a follow up paper that\n\n  for the VGG-19 network, there is a sweet spot around $500^2$ pixels for the\n  size of the input images, such that the stylisation is appealing but the content is\n  well-preserved.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def read_image(file, image_size=500):\n    image = Image.open(file)\n    image = Fv.resize(image, image_size)\n    return preprocessor(image)\n\n\ndef write_image(image, file):\n    image = postprocessor(image)\n    image.save(file)\n\n\ndef show_image(image, title=None, show_axis=False):\n    _, ax = plt.subplots()\n\n    ax.imshow(image)\n    if not show_axis:\n        ax.axis(\"off\")\n    if title is not None:\n        ax.set_title(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we put the previously defined encoder to use by creating the target content and\nstyle encodings for a different set of layers. The layer configuration is taken from\nGatys et. al.. Since we now know which layers we want to use, unused ones are removed\nfrom the encoder with the :meth:`~Encoder.trim` method.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}