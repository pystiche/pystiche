.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_galleries_examples_beginner_example_nst_without_pystiche.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_galleries_examples_beginner_example_nst_without_pystiche.py:


Neural Style Transfer without ``pystiche``
==========================================

This example showcases how a basic Neural Style Transfer (NST), i.e. image-based
optimization, could be performed without ``pystiche``.

.. note::

    This is an *example how to implement an NST* and **not** a
    *tutorial on how NST works*. As such, it will not explain why a specific choice was
    made or how a component works.

Setup
-----

We start this example by importing everything we need and setting the device we will
be working on.


.. code-block:: default
   :lineno-start: 22


    import itertools
    from collections import OrderedDict
    from os import path

    import matplotlib.pyplot as plt
    from PIL import Image

    import torch
    import torchvision
    from torch import nn, optim
    from torch.nn.functional import mse_loss
    from torchvision import transforms
    from torchvision.models import vgg19
    from torchvision.transforms.functional import resize

    print(f"I'm working with torch=={torch.__version__}")
    print(f"I'm working with torchvision=={torchvision.__version__}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"I'm working with {device}")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    I'm working with torch==1.5.0
    I'm working with torchvision==0.6.0
    I'm working with cuda




Perceptual Loss
---------------

The core component of every NST are the ``ContentLoss`` and the ``StyleLoss``.
Combined they make up the perceptual loss, i.e. the optimization criterion. In this
example we use the ``feature_reconstruction_loss`` introduced by Mahendran and
Vedaldi :cite:`MV2014` as well as the ``gram_loss`` introduced by Gatys, Ecker, and
Bethge :cite:`GEB2016` .


.. code-block:: default
   :lineno-start: 54



    def mean(sized):
        return sum(sized) / len(sized)


    def feature_reconstruction_loss(input, target):
        return mse_loss(input, target)


    class ContentLoss(nn.Module):
        def forward(self, input_encs, target_encs):
            layer_losses = [
                feature_reconstruction_loss(input, target)
                for input, target in zip(input_encs, target_encs)
            ]
            return mean(layer_losses)


    def gram_matrix(x, normalize=True):
        x = torch.flatten(x, 2)
        G = torch.bmm(x, x.transpose(1, 2))
        if normalize:
            return G / x.size()[-1]
        else:
            return G


    def gram_loss(input, target):
        return mse_loss(gram_matrix(input), gram_matrix(target))


    class StyleLoss(nn.Module):
        def forward(self, input_encs, target_encs):
            layer_losses = [
                gram_loss(input, target) for input, target in zip(input_encs, target_encs)
            ]
            return mean(layer_losses)









Multi-layer Encoder
-------------------
The ``ContentLoss`` and the ``StyleLoss`` operate on the encodings of an image rather
than on the image itself. These encodings are generated by a pretrained model. For
that purpose we define a ``MultiLayerEncoder`` with the given properties:

1. Given an image and a set of layers, the ``MultiLayerEncoder`` should return the
   encodings of every given layer.
2. Since the encodings have to be generated in every optimization step they should be
   calculated in a single forward pass to keep the processing costs low.
3. To reduce the static memory requirement, the ``MultiLayerEncoder`` should be
   ``trim`` mable in order to remove unused layers.


.. code-block:: default
   :lineno-start: 107



    class MultiLayerEncoder(nn.Sequential):
        def forward(self, image, layer_cfgs):
            storage = {}
            last_layer = self._find_last_layer(layer_cfgs)
            for layer, module in self.named_children():
                image = storage[layer] = module(image)
                if layer == last_layer:
                    break

            return [[storage[layer] for layer in layers] for layers in layer_cfgs]

        def children_names(self):
            for name, module in self.named_children():
                yield name

        def _find_last_layer(self, layer_cfgs):
            # find all unique requested layers
            req_layers = set(itertools.chain(*layer_cfgs))
            try:
                # find the deepest requested layer by indexing the layers within
                # the multi layer encoder
                children_names = list(self.children_names())
                return sorted(req_layers, key=children_names.index)[-1]
            except ValueError as error:
                layer = str(error).split()[0]
            raise ValueError(f"Layer {layer} is not part of the multi-layer encoder.")

        def trim(self, layer_cfgs):
            last_layer = self._find_last_layer(layer_cfgs)
            children_names = list(self.children_names())
            del self[children_names.index(last_layer) + 1 :]









The pretrained models the ``MultiLayerEncoder`` is based on are usually trained on
preprocessed images. In PyTorch all models expect images
`normalized <https://pytorch.org/docs/stable/torchvision/models.html>`_ by a
per-channel ``mean`` and standard deviation (``std``).


.. code-block:: default
   :lineno-start: 147



    class Normalize(nn.Module):
        def __init__(self, mean, std):
            super().__init__()
            self.register_buffer("mean", torch.tensor(mean).view(1, -1, 1, 1))
            self.register_buffer("std", torch.tensor(std).view(1, -1, 1, 1))

        def forward(self, image):
            return (image - self.mean) / self.std


    class TorchNormalize(Normalize):
        def __init__(self):
            super().__init__((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))









In a last step we need to specify the structure of ``MultiLayerEncoder``. For this
example we use a ``VGGMultiLayerEncoder`` based on the ``VGG19`` architeture
introduced by Simonyan and Zisserman :cite:`SZ2014`.


.. code-block:: default
   :lineno-start: 168



    class VGGMultiLayerEncoder(MultiLayerEncoder):
        def __init__(self, vgg_net, preprocessing=True):
            modules = OrderedDict()

            if preprocessing:
                modules["preprocessing"] = TorchNormalize()

            block = depth = 1
            for module in vgg_net.features.children():
                if isinstance(module, nn.Conv2d):
                    layer = f"conv{block}_{depth}"
                elif isinstance(module, nn.BatchNorm2d):
                    layer = f"bn{block}_{depth}"
                elif isinstance(module, nn.ReLU):
                    # without inplace=False the encodings of the previous layer would no
                    # longer be accessible after the ReLU layer is executed
                    module = nn.ReLU(inplace=False)
                    layer = f"relu{block}_{depth}"
                    # each ReLU layer increases the depth of the current block by one
                    depth += 1
                elif isinstance(module, nn.MaxPool2d):
                    layer = f"pool{block}"
                    # each max pooling layer marks the end of the current block
                    block += 1
                    depth = 1
                else:
                    # FIXME
                    raise RuntimeError

                modules[layer] = module

            super().__init__(modules)


    def vgg19_multi_layer_encoder(preprocessing=True):
        return VGGMultiLayerEncoder(vgg19(pretrained=True), preprocessing=preprocessing)


    multi_layer_encoder = vgg19_multi_layer_encoder().to(device)
    print(multi_layer_encoder)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    VGGMultiLayerEncoder(
      (preprocessing): TorchNormalize()
      (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1_1): ReLU()
      (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1_2): ReLU()
      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu2_1): ReLU()
      (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu2_2): ReLU()
      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_1): ReLU()
      (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_2): ReLU()
      (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_3): ReLU()
      (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_4): ReLU()
      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_1): ReLU()
      (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_2): ReLU()
      (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_3): ReLU()
      (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_4): ReLU()
      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_1): ReLU()
      (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_2): ReLU()
      (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_3): ReLU()
      (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_4): ReLU()
      (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )




Images
------

Before we can load the content and style image, we need to define some basic I/O
utilities. We use ``PIL`` for the file I/O and ``matplotlib.pyplot`` to show the
images.

At import a fake batch dimension is added to the images to be able to pass it through
the ``MultiLayerEncoder`` without further modification. This dimensions is upon
export removed again.


.. code-block:: default
   :lineno-start: 222


    import_from_pil = transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.Lambda(lambda x: x.unsqueeze(0)),
            transforms.Lambda(lambda x: x.to(device)),
        ]
    )

    export_to_pil = transforms.Compose(
        [
            transforms.Lambda(lambda x: x.cpu()),
            transforms.Lambda(lambda x: x.squeeze(0)),
            transforms.Lambda(lambda x: x.clamp(0.0, 1.0)),
            transforms.ToPILImage(),
        ]
    )


    def read_image(file, size=500):
        image = Image.open(file)
        image = resize(image, size)
        return import_from_pil(image)


    def write_image(image, file):
        image = export_to_pil(image)
        image.save(file)


    def show_image(image, title=None):
        _, ax = plt.subplots()
        ax.axis("off")
        if title is not None:
            ax.set_title(title)

        image = export_to_pil(image)
        ax.imshow(image)









.. note::

  By default all images will be resized to ``size=500`` pixels on the shorter edge.
  If you have more memory than X.X GB available you can increase this to obtain
  higher resolution results.

With the I/O utilities set up, we now load and show the images that will be used in
the NST.

.. note::

  By default the image files should be placed in ``../images/`` relative to this file.
  Adapt ``image_root`` if you want to use another directory.

.. note::

  You can download the default images here:

  - `Content image <https://github.com/pmeier/pystiche>`_
  - `Style image <https://github.com/pmeier/pystiche>`_


.. code-block:: default
   :lineno-start: 285


    # FIXME:
    # image_root = path.abspath(path.join("..", "images"))
    image_root = path.expanduser(path.join("~", ".cache", "pystiche"))
    content_image = read_image(path.join(image_root, "dancing.jpg"))
    show_image(content_image, title="Content image")





.. image:: /galleries/examples/beginner/images/sphx_glr_example_nst_without_pystiche_001.png
    :class: sphx-glr-single-img






.. code-block:: default
   :lineno-start: 294


    style_image = read_image(path.join(image_root, "picasso.jpg"))
    show_image(style_image, title="Style image")




.. image:: /galleries/examples/beginner/images/sphx_glr_example_nst_without_pystiche_002.png
    :class: sphx-glr-single-img





Neural Style Transfer
---------------------

At first we chose the ``content_layers`` and ``style_layers`` on which the encodings
will be are compared. With them we ``trim`` the ``multi_layer_encoder`` to remove
unused layers that otherwise occupy memory.

Afterwards we calculate the target content and style encodings and detach them from
the computation graph. This enables us to use them in every optimization step without
the need to recalculate them.


.. code-block:: default
   :lineno-start: 309


    content_layers = ("relu4_2",)
    style_layers = ("relu1_1", "relu2_1", "relu3_1", "relu4_1", "relu5_1")
    layer_cfgs = (content_layers, style_layers)

    multi_layer_encoder.trim(layer_cfgs)

    target_content_encs = multi_layer_encoder(content_image, (content_layers,))[0]
    target_content_encs = [enc.detach() for enc in target_content_encs]

    target_style_encs = multi_layer_encoder(style_image, (style_layers,))[0]
    target_style_encs = [enc.detach() for enc in target_style_encs]









We instantiate the ``ContentLoss`` and ``StyleLoss`` and select a corresponding
weight.


.. code-block:: default
   :lineno-start: 326


    content_criterion = ContentLoss()
    content_weight = 1e0

    style_criterion = StyleLoss()
    style_weight = 1e4









As a last preliminary step we create the input image and instantiate the optimizer.
We start from the ``content_image`` since this way the NST converges quickly.


.. code-block:: default
   :lineno-start: 337


    input_image = content_image.clone()
    show_image(input_image, "Input image")

    optimizer = optim.LBFGS([input_image.requires_grad_(True)], max_iter=1)





.. image:: /galleries/examples/beginner/images/sphx_glr_example_nst_without_pystiche_003.png
    :class: sphx-glr-single-img





.. note::

  If you want to start from a white noise image instead use

  .. code-block:: python

    input_image = torch.rand_like(content_image)

Finally we run the NST. The loss calculation has to happen inside a ``closure``
since the ``LBFGS`` optimizer could need to
`reevaluate it multiple times per optimization step <https://pytorch.org/docs/stable/optim.html#optimizer-step-closure>`_
. This structure is also valid for all other optimizers.


.. code-block:: default
   :lineno-start: 359


    num_steps = 500

    for step in range(1, num_steps + 1):

        def closure():
            optimizer.zero_grad()

            input_encs = multi_layer_encoder(input_image, layer_cfgs)
            input_content_encs, input_style_encs = input_encs

            content_score = content_criterion(input_content_encs, target_content_encs)
            content_score *= content_weight

            style_score = style_criterion(input_style_encs, target_style_encs)
            style_score *= style_weight

            loss = content_score + style_score
            loss.backward()

            if step % 50 == 0:
                print(f"Step {step}")
                print(f"Content loss: {content_score.item():.3e}")
                print(f"Style loss:   {style_score.item():.3e}")
                print("-----------------------")

            return loss

        optimizer.step(closure)

    output_image = input_image.detach()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Step 50
    Content loss: 2.492e+00
    Style loss:   8.268e+01
    -----------------------
    Step 100
    Content loss: 2.567e+00
    Style loss:   3.490e+01
    -----------------------
    Step 150
    Content loss: 2.600e+00
    Style loss:   1.943e+01
    -----------------------
    Step 200
    Content loss: 2.611e+00
    Style loss:   1.197e+01
    -----------------------
    Step 250
    Content loss: 2.608e+00
    Style loss:   8.815e+00
    -----------------------
    Step 300
    Content loss: 2.607e+00
    Style loss:   7.446e+00
    -----------------------
    Step 350
    Content loss: 2.600e+00
    Style loss:   6.713e+00
    -----------------------
    Step 400
    Content loss: 2.595e+00
    Style loss:   6.262e+00
    -----------------------
    Step 450
    Content loss: 2.591e+00
    Style loss:   5.946e+00
    -----------------------
    Step 500
    Content loss: 2.586e+00
    Style loss:   5.704e+00
    -----------------------




After the NST we show the resulting image and save it.


.. code-block:: default
   :lineno-start: 393


    show_image(output_image, title="Output image")
    write_image(output_image, path.join(image_root, "nst_without_pystiche.jpg"))




.. image:: /galleries/examples/beginner/images/sphx_glr_example_nst_without_pystiche_004.png
    :class: sphx-glr-single-img





Conclusion
----------

As hopefully has become clear even an NST in its simplest form requires quite a lot
of utilities and boilerplate code. This makes the it hard to maintain and keep bug
free as it is easy to lose track of everything.

Judging by the lines of code one could (falsely) conclude that the actual NST is just
an appendix. If you feel the same you can stop worrying now: in
`this follow-up example <>` we showcase how to achieve the same result with
``pystiche``.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  2.838 seconds)


.. _sphx_glr_download_galleries_examples_beginner_example_nst_without_pystiche.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: example_nst_without_pystiche.py <example_nst_without_pystiche.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: example_nst_without_pystiche.ipynb <example_nst_without_pystiche.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
