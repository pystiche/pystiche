{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nNeural Style Transfer without ``pystiche``\n==========================================\n\nThis example showcases how a basic Neural Style Transfer (NST), i.e. image-based\noptimization, could be performed without ``pystiche``.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This is an *example how to implement an NST* and **not** a\n    *tutorial on how NST works*. As such, it will not explain why a specific choice was\n    made or how a component works.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup\n-----\n\nWe start this example by importing everything we need and setting the device we will\nbe working on.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\nfrom collections import OrderedDict\nfrom os import path\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\nimport torchvision\nfrom torch import nn, optim\nfrom torch.nn.functional import mse_loss\nfrom torchvision import transforms\nfrom torchvision.models import vgg19\nfrom torchvision.transforms.functional import resize\n\nprint(f\"I'm working with torch=={torch.__version__}\")\nprint(f\"I'm working with torchvision=={torchvision.__version__}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"I'm working with {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perceptual Loss\n---------------\n\nThe core component of every NST are the ``ContentLoss`` and the ``StyleLoss``.\nCombined they make up the perceptual loss, i.e. the optimization criterion. In this\nexample we use the ``feature_reconstruction_loss`` introduced by Mahendran and\nVedaldi :cite:`MV2014` as well as the ``gram_loss`` introduced by Gatys, Ecker, and\nBethge :cite:`GEB2016` .\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def mean(sized):\n    return sum(sized) / len(sized)\n\n\ndef feature_reconstruction_loss(input, target):\n    return mse_loss(input, target)\n\n\nclass ContentLoss(nn.Module):\n    def forward(self, input_encs, target_encs):\n        layer_losses = [\n            feature_reconstruction_loss(input, target)\n            for input, target in zip(input_encs, target_encs)\n        ]\n        return mean(layer_losses)\n\n\ndef gram_matrix(x, normalize=True):\n    x = torch.flatten(x, 2)\n    G = torch.bmm(x, x.transpose(1, 2))\n    if normalize:\n        return G / x.size()[-1]\n    else:\n        return G\n\n\ndef gram_loss(input, target):\n    return mse_loss(gram_matrix(input), gram_matrix(target))\n\n\nclass StyleLoss(nn.Module):\n    def forward(self, input_encs, target_encs):\n        layer_losses = [\n            gram_loss(input, target) for input, target in zip(input_encs, target_encs)\n        ]\n        return mean(layer_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multi-layer Encoder\n-------------------\nThe ``ContentLoss`` and the ``StyleLoss`` operate on the encodings of an image rather\nthan on the image itself. These encodings are generated by a pretrained model. For\nthat purpose we define a ``MultiLayerEncoder`` with the given properties:\n\n1. Given an image and a set of layers, the ``MultiLayerEncoder`` should return the\n   encodings of every given layer.\n2. Since the encodings have to be generated in every optimization step they should be\n   calculated in a single forward pass to keep the processing costs low.\n3. To reduce the static memory requirement, the ``MultiLayerEncoder`` should be\n   ``trim`` mable in order to remove unused layers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MultiLayerEncoder(nn.Sequential):\n    def forward(self, image, layer_cfgs):\n        storage = {}\n        last_layer = self._find_last_layer(layer_cfgs)\n        for layer, module in self.named_children():\n            image = storage[layer] = module(image)\n            if layer == last_layer:\n                break\n\n        return [[storage[layer] for layer in layers] for layers in layer_cfgs]\n\n    def children_names(self):\n        for name, module in self.named_children():\n            yield name\n\n    def _find_last_layer(self, layer_cfgs):\n        # find all unique requested layers\n        req_layers = set(itertools.chain(*layer_cfgs))\n        try:\n            # find the deepest requested layer by indexing the layers within\n            # the multi layer encoder\n            children_names = list(self.children_names())\n            return sorted(req_layers, key=children_names.index)[-1]\n        except ValueError as error:\n            layer = str(error).split()[0]\n        raise ValueError(f\"Layer {layer} is not part of the multi-layer encoder.\")\n\n    def trim(self, layer_cfgs):\n        last_layer = self._find_last_layer(layer_cfgs)\n        children_names = list(self.children_names())\n        del self[children_names.index(last_layer) + 1 :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pretrained models the ``MultiLayerEncoder`` is based on are usually trained on\npreprocessed images. In PyTorch all models expect images\n`normalized <https://pytorch.org/docs/stable/torchvision/models.html>`_ by a\nper-channel ``mean`` and standard deviation (``std``).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Normalize(nn.Module):\n    def __init__(self, mean, std):\n        super().__init__()\n        self.register_buffer(\"mean\", torch.tensor(mean).view(1, -1, 1, 1))\n        self.register_buffer(\"std\", torch.tensor(std).view(1, -1, 1, 1))\n\n    def forward(self, image):\n        return (image - self.mean) / self.std\n\n\nclass TorchNormalize(Normalize):\n    def __init__(self):\n        super().__init__((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a last step we need to specify the structure of ``MultiLayerEncoder``. For this\nexample we use a ``VGGMultiLayerEncoder`` based on the ``VGG19`` architeture\nintroduced by Simonyan and Zisserman :cite:`SZ2014`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class VGGMultiLayerEncoder(MultiLayerEncoder):\n    def __init__(self, vgg_net, preprocessing=True):\n        modules = OrderedDict()\n\n        if preprocessing:\n            modules[\"preprocessing\"] = TorchNormalize()\n\n        block = depth = 1\n        for module in vgg_net.features.children():\n            if isinstance(module, nn.Conv2d):\n                layer = f\"conv{block}_{depth}\"\n            elif isinstance(module, nn.BatchNorm2d):\n                layer = f\"bn{block}_{depth}\"\n            elif isinstance(module, nn.ReLU):\n                # without inplace=False the encodings of the previous layer would no\n                # longer be accessible after the ReLU layer is executed\n                module = nn.ReLU(inplace=False)\n                layer = f\"relu{block}_{depth}\"\n                # each ReLU layer increases the depth of the current block by one\n                depth += 1\n            elif isinstance(module, nn.MaxPool2d):\n                layer = f\"pool{block}\"\n                # each max pooling layer marks the end of the current block\n                block += 1\n                depth = 1\n            else:\n                # FIXME\n                raise RuntimeError\n\n            modules[layer] = module\n\n        super().__init__(modules)\n\n\ndef vgg19_multi_layer_encoder(preprocessing=True):\n    return VGGMultiLayerEncoder(vgg19(pretrained=True), preprocessing=preprocessing)\n\n\nmulti_layer_encoder = vgg19_multi_layer_encoder().to(device)\nprint(multi_layer_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Images\n------\n\nBefore we can load the content and style image, we need to define some basic I/O\nutilities. We use ``PIL`` for the file I/O and ``matplotlib.pyplot`` to show the\nimages.\n\nAt import a fake batch dimension is added to the images to be able to pass it through\nthe ``MultiLayerEncoder`` without further modification. This dimensions is upon\nexport removed again.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import_from_pil = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x.unsqueeze(0)),\n        transforms.Lambda(lambda x: x.to(device)),\n    ]\n)\n\nexport_to_pil = transforms.Compose(\n    [\n        transforms.Lambda(lambda x: x.cpu()),\n        transforms.Lambda(lambda x: x.squeeze(0)),\n        transforms.Lambda(lambda x: x.clamp(0.0, 1.0)),\n        transforms.ToPILImage(),\n    ]\n)\n\n\ndef read_image(file, size=500):\n    image = Image.open(file)\n    image = resize(image, size)\n    return import_from_pil(image)\n\n\ndef write_image(image, file):\n    image = export_to_pil(image)\n    image.save(file)\n\n\ndef show_image(image, title=None):\n    _, ax = plt.subplots()\n    ax.axis(\"off\")\n    if title is not None:\n        ax.set_title(title)\n\n    image = export_to_pil(image)\n    ax.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>By default all images will be resized to ``size=500`` pixels on the shorter edge.\n  If you have more memory than X.X GB available you can increase this to obtain\n  higher resolution results.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the I/O utilities set up, we now load and show the images that will be used in\nthe NST.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>By default the image files should be placed in ``../images/`` relative to this file.\n  Adapt ``image_root`` if you want to use another directory.</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You can download the default images here:\n\n  - `Content image <https://github.com/pmeier/pystiche>`_\n  - `Style image <https://github.com/pmeier/pystiche>`_</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# FIXME:\n# image_root = path.abspath(path.join(\"..\", \"images\"))\nimage_root = path.expanduser(path.join(\"~\", \".cache\", \"pystiche\"))\ncontent_image = read_image(path.join(image_root, \"dancing.jpg\"))\nshow_image(content_image, title=\"Content image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "style_image = read_image(path.join(image_root, \"picasso.jpg\"))\nshow_image(style_image, title=\"Style image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neural Style Transfer\n---------------------\n\nAt first we chose the ``content_layers`` and ``style_layers`` on which the encodings\nwill be are compared. With them we ``trim`` the ``multi_layer_encoder`` to remove\nunused layers that otherwise occupy memory.\n\nAfterwards we calculate the target content and style encodings and detach them from\nthe computation graph. This enables us to use them in every optimization step without\nthe need to recalculate them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "content_layers = (\"relu4_2\",)\nstyle_layers = (\"relu1_1\", \"relu2_1\", \"relu3_1\", \"relu4_1\", \"relu5_1\")\nlayer_cfgs = (content_layers, style_layers)\n\nmulti_layer_encoder.trim(layer_cfgs)\n\ntarget_content_encs = multi_layer_encoder(content_image, (content_layers,))[0]\ntarget_content_encs = [enc.detach() for enc in target_content_encs]\n\ntarget_style_encs = multi_layer_encoder(style_image, (style_layers,))[0]\ntarget_style_encs = [enc.detach() for enc in target_style_encs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We instantiate the ``ContentLoss`` and ``StyleLoss`` and select a corresponding\nweight.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "content_criterion = ContentLoss()\ncontent_weight = 1e0\n\nstyle_criterion = StyleLoss()\nstyle_weight = 1e4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a last preliminary step we create the input image and instantiate the optimizer.\nWe start from the ``content_image`` since this way the NST converges quickly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_image = content_image.clone()\nshow_image(input_image, \"Input image\")\n\noptimizer = optim.LBFGS([input_image.requires_grad_(True)], max_iter=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If you want to start from a white noise image instead use\n\n  .. code-block:: python\n\n    input_image = torch.rand_like(content_image)</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we run the NST. The loss calculation has to happen inside a ``closure``\nsince the ``LBFGS`` optimizer could need to\n`reevaluate it multiple times per optimization step <https://pytorch.org/docs/stable/optim.html#optimizer-step-closure>`_\n. This structure is also valid for all other optimizers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_steps = 500\n\nfor step in range(1, num_steps + 1):\n\n    def closure():\n        optimizer.zero_grad()\n\n        input_encs = multi_layer_encoder(input_image, layer_cfgs)\n        input_content_encs, input_style_encs = input_encs\n\n        content_score = content_criterion(input_content_encs, target_content_encs)\n        content_score *= content_weight\n\n        style_score = style_criterion(input_style_encs, target_style_encs)\n        style_score *= style_weight\n\n        loss = content_score + style_score\n        loss.backward()\n\n        if step % 50 == 0:\n            print(f\"Step {step}\")\n            print(f\"Content loss: {content_score.item():.3e}\")\n            print(f\"Style loss:   {style_score.item():.3e}\")\n            print(\"-----------------------\")\n\n        return loss\n\n    optimizer.step(closure)\n\noutput_image = input_image.detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the NST we show the resulting image and save it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "show_image(output_image, title=\"Output image\")\nwrite_image(output_image, path.join(image_root, \"nst_without_pystiche.jpg\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n----------\n\nAs hopefully has become clear even an NST in its simplest form requires quite a lot\nof utilities and boilerplate code. This makes the it hard to maintain and keep bug\nfree as it is easy to lose track of everything.\n\nJudging by the lines of code one could (falsely) conclude that the actual NST is just\nan appendix. If you feel the same you can stop worrying now: in\n`this follow-up example <>` we showcase how to achieve the same result with\n``pystiche``.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}