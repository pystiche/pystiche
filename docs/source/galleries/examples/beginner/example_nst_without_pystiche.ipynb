{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nNeural Style Transfer without ``pystiche``\n==========================================\n\nThis example showcases how a basic Neural Style Transfer (NST), i.e. image-based\noptimization, could be performed without ``pystiche``.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This is an *example how to implement an NST* and **not** a\n    *tutorial on how NST works*. As such, it will not explain why a specific choice was\n    made or how a component works.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup\n-----\n\nWe start this example by importing everything we need and setting the device we will\nbe working on.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\nfrom collections import OrderedDict\nfrom os import path\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\nimport torchvision\nfrom torch import nn, optim\nfrom torch.nn.functional import mse_loss\nfrom torchvision import transforms\nfrom torchvision.models import vgg19\nfrom torchvision.transforms.functional import resize\n\nprint(f\"I'm working with torch=={torch.__version__}\")\nprint(f\"I'm working with torchvision=={torchvision.__version__}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"I'm working with {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The core component of different NSTs is the perceptual loss, which is used as\noptimization criterion. The perceptual loss is usually, and also for this example,\ncalculated on features maps also called encodings. These encodings are generated from\ndifferent layers of a Convolutional Neural Net (CNN) also called encoder.\n\nA common implementation strategy is to *weave in* transparent loss layers into the\nencoder. These loss layers are called transparent since they simply pass the input\nthrough only store the generated loss in themselves. While this is simple to\nimplement, this practice has two downsides:\n\n1. The calculated score is part of the current but has to be stored inside the layer.\n   This is generally not recommended.\n2. While the encoder is a part of the perceptual loss, it itself does not generate\n   it. One should be able to use the same encoder with a different perceptual loss\n   without modification.\n\nThus, this example (and ``pystiche``) follows a different approach and separates the\nencoder and the perceptual loss into individual entities.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multi-layer Encoder\n-------------------\n\nIn a first step we define a ``MultiLayerEncoder`` that should have the following\nproperties:\n\n1. Given an image and a set of layers, the ``MultiLayerEncoder`` should return the\n   encodings of every given layer.\n2. Since the encodings have to be generated in every optimization step they should be\n   calculated in a single forward pass to keep the processing costs low.\n3. To reduce the static memory requirement, the ``MultiLayerEncoder`` should be\n   ``trim`` mable in order to remove unused layers.\n\nWe achieve the main functionality by subclassing ``nn.Sequential`` and define a\ncustom ``forward`` method, i.e. different behavior if called. Besides the image it\nalso takes an iterable ``layer_cfgs`` containing multiple sequences of ``layers``. In\nthe method body we first find the ``deepest_layer`` that was requested. Subsequently,\nwe calculate and store all encodings of the ``image`` up to that layer. Finally we\ncan return all requested encodings without processing the same layer twice.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MultiLayerEncoder(nn.Sequential):\n    def forward(self, image, *layer_cfgs):\n        storage = {}\n        deepest_layer = self._find_deepest_layer(*layer_cfgs)\n        for layer, module in self.named_children():\n            image = storage[layer] = module(image)\n            if layer == deepest_layer:\n                break\n\n        return [[storage[layer] for layer in layers] for layers in layer_cfgs]\n\n    def children_names(self):\n        for name, module in self.named_children():\n            yield name\n\n    def _find_deepest_layer(self, *layer_cfgs):\n        # find all unique requested layers\n        req_layers = set(itertools.chain(*layer_cfgs))\n        try:\n            # find the deepest requested layer by indexing the layers within\n            # the multi layer encoder\n            children_names = list(self.children_names())\n            return sorted(req_layers, key=children_names.index)[-1]\n        except ValueError as error:\n            layer = str(error).split()[0]\n        raise ValueError(f\"Layer {layer} is not part of the multi-layer encoder.\")\n\n    def trim(self, *layer_cfgs):\n        deepest_layer = self._find_deepest_layer(*layer_cfgs)\n        children_names = list(self.children_names())\n        del self[children_names.index(deepest_layer) + 1 :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pretrained models the ``MultiLayerEncoder`` is based on are usually trained on\npreprocessed images. In PyTorch all models expect images\n`normalized <https://pytorch.org/docs/stable/torchvision/models.html>`_ by a\nper-channel ``mean`` and standard deviation (``std``). To include this into a\n``MultiLayerEncoder``, we implement this as ``nn.Module``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Normalize(nn.Module):\n    def __init__(self, mean, std):\n        super().__init__()\n        self.register_buffer(\"mean\", torch.tensor(mean).view(1, -1, 1, 1))\n        self.register_buffer(\"std\", torch.tensor(std).view(1, -1, 1, 1))\n\n    def forward(self, image):\n        return (image - self.mean) / self.std\n\n\nclass TorchNormalize(Normalize):\n    def __init__(self):\n        super().__init__((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a last step we need to specify the structure of the ``MultiLayerEncoder``. In this\nexample we use a ``VGGMultiLayerEncoder`` based on the ``VGG19`` CNN introduced by\nSimonyan and Zisserman :cite:`SZ2014`.\n\nWe only include the feature extraction stage (``vgg_net.features``), i.e. the\nconvolutional stage, since classifier stage (``vgg_net.classifier``) only accepts\nfeature maps of a single size.\n\nFor our convenience we rename the layers in the same scheme the authors used instead\nof keeping the consecutive index of a default ``nn.Sequential``. The first layer\nhowever is the ``TorchNormalize``  as defined above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class VGGMultiLayerEncoder(MultiLayerEncoder):\n    def __init__(self, vgg_net):\n        modules = OrderedDict(((\"preprocessing\", TorchNormalize()),))\n\n        block = depth = 1\n        for module in vgg_net.features.children():\n            if isinstance(module, nn.Conv2d):\n                layer = f\"conv{block}_{depth}\"\n            elif isinstance(module, nn.BatchNorm2d):\n                layer = f\"bn{block}_{depth}\"\n            elif isinstance(module, nn.ReLU):\n                # without inplace=False the encodings of the previous layer would no\n                # longer be accessible after the ReLU layer is executed\n                module = nn.ReLU(inplace=False)\n                layer = f\"relu{block}_{depth}\"\n                # each ReLU layer increases the depth of the current block by one\n                depth += 1\n            elif isinstance(module, nn.MaxPool2d):\n                layer = f\"pool{block}\"\n                # each max pooling layer marks the end of the current block\n                block += 1\n                depth = 1\n            else:\n                msg = f\"Type {type(module)} is not part of the VGG architecture.\"\n                raise RuntimeError(msg)\n\n            modules[layer] = module\n\n        super().__init__(modules)\n\n\ndef vgg19_multi_layer_encoder():\n    return VGGMultiLayerEncoder(vgg19(pretrained=True))\n\n\nmulti_layer_encoder = vgg19_multi_layer_encoder().to(device)\nprint(multi_layer_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perceptual Loss\n---------------\n\nIn order to calculate the perceptual loss, i.e. the optimization criterion we define\na ``MultiLayerLoss`` to have a convenient interface. This will be subclassed later by\nthe ``ContentLoss`` and ``StyleLoss``.\n\nIf called with a sequence of ``\u00ecnput_encs`` the ``MultiLayerLoss`` should calculate\nlayerwise scores together with the corresponding ``target_encs``. For that a\n``MultiLayerLoss`` needs the ability to store the ``target_encs`` so that they can be\nreused for every call. The individual layer scores should be averaged by the number\nof encodings and finally weighted by a ``score_weight``.\n\nTo achieve this we subclass ``nn.Module``. The ``target_encs`` are stored as buffers,\nsince they are not trainable parameters. The actual functionality has to be defined\nin ``calculate_score`` by a subclass.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def mean(sized):\n    return sum(sized) / len(sized)\n\n\nclass MultiLayerLoss(nn.Module):\n    def __init__(self, score_weight=1e0):\n        super().__init__()\n        self.score_weight = score_weight\n        self._numel_target_encs = 0\n\n    def _target_enc_name(self, idx):\n        return f\"_target_encs_{idx}\"\n\n    def set_target_encs(self, target_encs):\n        self._numel_target_encs = len(target_encs)\n        for idx, enc in enumerate(target_encs):\n            self.register_buffer(self._target_enc_name(idx), enc.detach())\n\n    @property\n    def target_encs(self):\n        return tuple(\n            [\n                getattr(self, self._target_enc_name(idx))\n                for idx in range(self._numel_target_encs)\n            ]\n        )\n\n    def forward(self, input_encs):\n        if len(input_encs) != self._numel_target_encs:\n            msg = (\n                f\"The number of given input encodings and stored target encodings \"\n                f\"does not match: {len(input_encs)} != {self._numel_target_encs}\"\n            )\n            raise RuntimeError(msg)\n\n        layer_losses = [\n            self.calculate_score(input, target)\n            for input, target in zip(input_encs, self.target_encs)\n        ]\n        return mean(layer_losses) * self.score_weight\n\n    def calculate_score(self, input, target):\n        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we use the ``feature_reconstruction_loss`` introduced by Mahendran\nand Vedaldi :cite:`MV2014` as ``ContentLoss`` as well as the ``gram_loss`` introduced\nby Gatys, Ecker, and Bethge :cite:`GEB2016` as ``StyleLoss``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def feature_reconstruction_loss(input, target):\n    return mse_loss(input, target)\n\n\nclass ContentLoss(MultiLayerLoss):\n    def calculate_score(self, input, target):\n        return feature_reconstruction_loss(input, target)\n\n\ndef channelwise_gram_matrix(x, normalize=True):\n    x = torch.flatten(x, 2)\n    G = torch.bmm(x, x.transpose(1, 2))\n    if normalize:\n        return G / x.size()[-1]\n    else:\n        return G\n\n\ndef gram_loss(input, target):\n    return mse_loss(channelwise_gram_matrix(input), channelwise_gram_matrix(target))\n\n\nclass StyleLoss(MultiLayerLoss):\n    def calculate_score(self, input, target):\n        return gram_loss(input, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Images\n------\n\nBefore we can load the content and style image, we need to define some basic I/O\nutilities. We use ``PIL`` for the file I/O and ``matplotlib.pyplot`` to show the\nimages.\n\nAt import a fake batch dimension is added to the images to be able to pass it through\nthe ``MultiLayerEncoder`` without further modification. This dimensions is removed\nagain upon export.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import_from_pil = transforms.Compose(\n    (\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x.unsqueeze(0)),\n        transforms.Lambda(lambda x: x.to(device)),\n    )\n)\n\nexport_to_pil = transforms.Compose(\n    (\n        transforms.Lambda(lambda x: x.cpu()),\n        transforms.Lambda(lambda x: x.squeeze(0)),\n        transforms.Lambda(lambda x: x.clamp(0.0, 1.0)),\n        transforms.ToPILImage(),\n    )\n)\n\n\ndef read_image(file, size=500):\n    image = Image.open(file)\n    image = resize(image, size)\n    return import_from_pil(image)\n\n\ndef write_image(image, file):\n    image = export_to_pil(image)\n    image.save(file)\n\n\ndef show_image(image, title=None):\n    _, ax = plt.subplots()\n    ax.axis(\"off\")\n    if title is not None:\n        ax.set_title(title)\n\n    image = export_to_pil(image)\n    ax.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>By default all images will be resized to ``size=500`` pixels on the shorter edge.\n  If you have more memory than X.X GB available you can increase this to obtain\n  higher resolution results.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the I/O utilities set up, we now load and show the images that will be used in\nthe NST.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# FIXME:\n# image_root = path.abspath(path.join(\"..\", \"images\"))\nimage_root = path.expanduser(path.join(\"~\", \".cache\", \"pystiche\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>By default the image files should be placed in ``../images/`` relative to this file.\n  Adapt ``image_root`` if you want to use another directory.</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You can download the default images here:\n\n  - `Content image <https://free-images.com/md/71c4/bird_wildlife_australian_bird.jpg>`_\n  - `Style image <https://cdn.pixabay.com/photo/2017/07/03/20/17/abstract-2468874_960_720.jpg>`_</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "content_image = read_image(path.join(image_root, \"bird.jpg\"))\nshow_image(content_image, title=\"Content image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "style_image = read_image(path.join(image_root, \"paint.jpg\"))\nshow_image(style_image, title=\"Style image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neural Style Transfer\n---------------------\n\nAt first we chose the ``content_layers`` and ``style_layers`` on which the encodings\nwill be are compared. With them we ``trim`` the ``multi_layer_encoder`` to remove\nunused layers that otherwise occupy memory.\n\nAfterwards we calculate the target content and style encodings. The calculation is\nperformed without a gradient since the gradient of the target encodings is not needed\nfor the optimization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "content_layers = (\"relu4_2\",)\nstyle_layers = (\"relu1_1\", \"relu2_1\", \"relu3_1\", \"relu4_1\", \"relu5_1\")\n\nmulti_layer_encoder.trim(content_layers, style_layers)\n\nwith torch.no_grad():\n    target_content_encs = multi_layer_encoder(content_image, content_layers)[0]\n    target_style_encs = multi_layer_encoder(style_image, style_layers)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next up, we instantiate the ``ContentLoss`` and ``StyleLoss`` with a corresponding\nweight. Afterwards we store the previously calculated target encodings.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "content_weight = 1e0\ncontent_criterion = ContentLoss(score_weight=content_weight)\ncontent_criterion.set_target_encs(target_content_encs)\n\nstyle_weight = 1e3\nstyle_criterion = StyleLoss(score_weight=style_weight)\nstyle_criterion.set_target_encs(target_style_encs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start NST from the ``content_image`` since this way it converges quickly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_image = content_image.clone()\nshow_image(input_image, \"Input image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If you want to start from a white noise image instead use\n\n  .. code-block:: python\n\n    input_image = torch.rand_like(content_image)</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a last preliminary step we create the optimizer that will be performing the NST.\nSince we want to adapt the pixels of the ``input_image`` directly, we pass it as\noptimization parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = optim.LBFGS([input_image.requires_grad_(True)], max_iter=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we run the NST. The loss calculation has to happen inside a ``closure``\nsince the ``LBFGS`` optimizer could need to\n`reevaluate it multiple times per optimization step <https://pytorch.org/docs/stable/optim.html#optimizer-step-closure>`_\n. This structure is also valid for all other optimizers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_steps = 500\n\nfor step in range(1, num_steps + 1):\n\n    def closure():\n        optimizer.zero_grad()\n\n        input_encs = multi_layer_encoder(input_image, content_layers, style_layers)\n        input_content_encs, input_style_encs = input_encs\n\n        content_score = content_criterion(input_content_encs)\n        style_score = style_criterion(input_style_encs)\n\n        perceptual_loss = content_score + style_score\n        perceptual_loss.backward()\n\n        if step % 50 == 0:\n            print(f\"Step {step}\")\n            print(f\"Content loss: {content_score.item():.3e}\")\n            print(f\"Style loss:   {style_score.item():.3e}\")\n            print(\"-----------------------\")\n\n        return perceptual_loss\n\n    optimizer.step(closure)\n\noutput_image = input_image.detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the NST we show the resulting image and save it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "show_image(output_image, title=\"Output image\")\nwrite_image(output_image, path.join(image_root, \"nst_without_pystiche.jpg\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n----------\n\nAs hopefully has become clear even an NST in its simplest form requires quite a lot\nof utilities and boilerplate code. This makes the it hard to maintain and keep bug\nfree as it is easy to lose track of everything.\n\nJudging by the lines of code one could (falsely) conclude that the actual NST is just\nan appendix. If you feel the same you can stop worrying now: in\n`this follow-up example <>` we showcase how to achieve the same result with\n``pystiche``.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}